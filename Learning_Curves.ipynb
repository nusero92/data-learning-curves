{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ This exercise consists of using Learning Curves to diagnose the performance of a model in regards to **bias** and **variance**, as well as spotting and correcting **underfitting** and **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Load the `NBA.csv` dataset into this notebook as a pandas dataframe, and display its first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è You can read a detailed description of the dataset in the challenge README. Make sure to refer to it throughout the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Cross validate a Linear Regression model meant to predict player win rating according to minutes played (`mp`). Save the mean of the scores as `cv_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('cv_score',\n",
    "                         score = cv_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Curves"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves are used to diagnose the performance of the model in more depth.\n",
    " \n",
    "‚ùì Plot the learning curves of the model ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html)). \n",
    "\n",
    "For the training sizes, you should start with **100** rows and increment by **100** until 80% of the dataset is used in training (**3200**). Hence, you should end up with **32** slices. [np.arange](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) can help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì How would you interpret the learning curves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Unfold this cell to see our interpretation </summary>   \n",
    "    \n",
    "<br/>\n",
    "    \n",
    "We are observing **underfitting**\n",
    "\n",
    "üëâ The curves should have converged at a low score (be conscious of the scale: sometimes they look far apart, but their score is very close! You can try changing <code>plt.ylim()</code> to have a clearer view)\n",
    "- Training score has gone down substantially training size increased\n",
    "- Testing score has barely gone up - less than 1%! - even with 80% of the dataset being taken for training.\n",
    "\n",
    "üëâ There are two typical reasons that cause underfitting:\n",
    "- The model is **too simple** to learn the patterns of the data\n",
    "- The model needs **more features** to get better at predicting player win rating\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, as we saw the model performance doesn't seem optimal. We can try fixing it by adding more features - let's go with the ones our NBA fantasty league friend suggested!\n",
    "\n",
    "‚ùì Cross validate a model made to predict player win rating with:\n",
    "- Minutes played (`mp`)\n",
    "- Possessions (`poss`)\n",
    "- Defense/offense ratio (`do_ratio`)\n",
    "- Pacing\n",
    "\n",
    "Save the new cross-validated score under a variable named `score_added_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è The performance of the model should have increased! Adding features provides the model with additional information from which to learn and with which to model the pattern of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learning Curves 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the learning curves of the new model to evaluate its performance further (you can play with [plt.ylim()](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ylim.html) to make the curves more obvious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì How would you interpret the learning curves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Unfold this cell to see our interpretation </summary>   \n",
    "    \n",
    "<br/>\n",
    "    \n",
    "There's an improvement, but there still appears to be some **underfitting**\n",
    "\n",
    "üëâ The curves should have converged, but the training score still drops substantially more than the testing score increases.\n",
    "\n",
    "üëâ Adding more features helped a little, but didn't fully solve the underfitting problem. So we can look into the other typical case:\n",
    "- The model is **too simple** to learn the patterns of the data\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving Linear Model Fit with Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done the best we could with the features from our dataset, but we're still seeing some signs of underfitting.\n",
    "\n",
    "It might be that relying on linear combinations of features makes our model **too simple** for the relationship between win rating and the features of players - we can try to fix that with some **feature engineering**. üõ†Ô∏è\n",
    "\n",
    "üîç Most of our ability to explain/predict player win rating has come from the minutes played (`mp`) feature. Let's look into this feature more.\n",
    "\n",
    "üëá Plot a scatterplot of the relationship between these two columns. Feel free to use seaborn or matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=df, x='mp', y='win_rating', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Let's train a `LinearRegression` model with `mp` and `win_rating` with a **holdout**.\n",
    "\n",
    "üëá Let's plot the learned regression line on the same plot to see how well it fits the data. Remember, you can extract the coefficients of a trained linear model with `coef_` and `intercept_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# training the model\n",
    "model = LinearRegression()\n",
    "\n",
    "X = df[['mp']]\n",
    "y = df['win_rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# scoring the model\n",
    "lin_reg_score = model.score(X_test, y_test)\n",
    "print(\"Model R2:\", lin_reg_score)\n",
    "\n",
    "# extracting the coefficients and regression function\n",
    "regression = model.coef_[0] * df['mp'] + model.intercept_\n",
    "\n",
    "# plotting the data and learned regression function\n",
    "sns.scatterplot(data=df, x='mp', y='win_rating', alpha=0.5)\n",
    "plt.plot(df['mp'], regression, color='red', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Do you see what could be causing our model to underfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Unfold this cell to see our interpretation </summary>   \n",
    "    \n",
    "<br/>\n",
    "    \n",
    "It looks like we're trying to fit a straight line learned by the Linear Regression model on data that is distributed *curvilinearly*.\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try improving the model by adding [**Polynomial Features**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures).\n",
    "\n",
    "Polynomial features are products of existing features of a set degree. For example, if we have two features - `a` and `b` - and we add degree-2 polynomial features, we'd end up with a feature set of [`a`, `b`, `a`$^2$, `a` $*$ `b`, `b`$^2$].\n",
    "\n",
    "Thanks to the exponents created by sklearn's `PolynomialFeatures`, we can train a model to closer represent curvilinear relationships, like the one observed between players' win ratings and minutes played.\n",
    "\n",
    "Let's try it! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Let's update our feature set `X` with degree-2 polynomial features. Check the [example in the sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#Examples:~:text=linear_model/plot_polynomial_interpolation.py-,Examples,-%3E%3E%3E) to see how to transform the data - the first example below is on us üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False) # we don't want to add a column of 1's\n",
    "X_poly = polynomial_features.fit_transform(X)\n",
    "\n",
    "X_poly = pd.DataFrame(X_poly) # turning it back into a DataFrame for easier manipulation\n",
    "X_poly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Note that column names disappeared due to the transformation - feel free to update them, or just keep in mind that the first column is our original `mp` and the new column - `mp`$^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Let's check if adding a `degree=2` polynomial feature helps the model better represent the relationship between minutes played and a player's win rating.\n",
    "\n",
    "üëá Run the cell below to see the new regression line, created by a model trained on degree-2 polynomial features, meaning `mp` and `mp`$^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values('mp')\n",
    "\n",
    "X = sorted_df[['mp']]\n",
    "y = sorted_df['win_rating']\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "predictions = model.predict(X_poly)\n",
    "\n",
    "sns.scatterplot(x=X['mp'], y=y, alpha=0.5)\n",
    "plt.plot(X['mp'], predictions, linewidth=3, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° The new regression line seems to be a better fit for our data - we're on the right track!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Picking the best number of degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¢ Let's go back to our **full feature set** - `['mp', 'poss', 'do_ratio', 'pacing']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that Polynomial Features on `mp` improves the model, but how about the rest of the dataset? ü§î\n",
    "\n",
    "‚ùì Cross-validate a model trained on `degree=2` polynomial features of the whole dataset. How does the score change from previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model score should have improved substantially! But how do we know if `degree=2` is the best one, and not 3, or 5, or 10...? Giving our model more degrees of freedom should only make the model better, right? ü§î\n",
    "\n",
    "‚ùì Do cross-validation with `degree` of polynomial features ranging from **1** to **10**. Save all the scores, then plot them with their respective degrees to pinpoint the best result.\n",
    "\n",
    "üï∞Ô∏è **NOTE:** it will take a while to run the full loop, as higher number of polynomial degrees creates exponentially more features. \n",
    "\n",
    "‚ùì While it runs, think about how many features in total will your dataset have with `degree=10`?\n",
    "\n",
    "</br>\n",
    "\n",
    "<details>\n",
    "<summary> üÜò Click here for the answer </summary>   \n",
    "\n",
    "For every feature you will create 10 new features of varying powers, that's **44** features already. Then products of all possible combinations of features are added - resulting in **1000** features.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Which polynomial feature degree is best suited to predict NBA players' win rating?\n",
    "\n",
    "<br/>\n",
    "\n",
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Click here to validate your answer </summary>   \n",
    "    \n",
    "‚òùÔ∏è You should be able to see that `degree=2` does give us the best score - now up to **0.87** after adding back the rest of the features!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üò± But what happens after `degree=5` - with *even more* information the model results start going down??\n",
    "\n",
    "üëá Plot the learning curves of a model with `degree=5` polynomial features to try and pinpoint the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our X to include polynomial features\n",
    "poly_features = PolynomialFeatures(degree=5, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    estimator = LinearRegression(),\n",
    "    X = X_poly, \n",
    "    y = y, \n",
    "    train_sizes = train_sizes, \n",
    "    cv = 5\n",
    ")\n",
    "\n",
    "# Take the mean of cross-validated train scores and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves!\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, label = 'Test score')\n",
    "plt.ylabel('r2 score', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves', fontsize = 18, y = 1.03)\n",
    "plt.ylim(0,1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì How would you interpret these learning curves?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> ‚ÑπÔ∏è Unfold this cell to see our interpretation </summary>   \n",
    "    \n",
    "<br/>\n",
    "    \n",
    "We see obvious **overfitting**!\n",
    "    \n",
    "üëâ The curves don't converge, the model is overfitting the training data with a training score (`~0.9`) that stays substantially higher than the testing score (`~0.7`).\n",
    "    \n",
    "üëâ By adding degree-5 polynomial features, there appears too much \"noise\" that the model pays attention to and the learned coefficients do not represent reality any more.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reducing Training Set Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Now that we've found the best model score we could get, let's see if we can afford to train the model on less of data to save computation resources.\n",
    "\n",
    "üëá Let's plot the learning curves of our best model - all features with `degree=2` polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training size slices\n",
    "train_sizes = np.linspace(100, 3200, 32, dtype='int')\n",
    "\n",
    "X_poly = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = LinearRegression(),\n",
    "                                                              X = X_poly, \n",
    "                                                              y = y, \n",
    "                                                              train_sizes = train_sizes, \n",
    "                                                              cv = 5)\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves!\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'training score')\n",
    "plt.plot(train_sizes, test_scores_mean, label = 'test score')\n",
    "plt.ylabel('r2 score', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves', fontsize = 18, y = 1.03)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìLooking at the new learning curves, how many training examples are sufficient for the model to learn the patterns of the dataset?\n",
    "\n",
    "üëá Run the cell below after you've come up with the answer to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the learning curves\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_sizes, train_scores_mean, label = 'training score')\n",
    "plt.plot(train_sizes, test_scores_mean, label = 'test score')\n",
    "plt.ylabel('r2 score', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves', fontsize = 18, y = 1.03)\n",
    "\n",
    "# Plotting a line where difference of train and test score becomes <1%\n",
    "plt.axvline(1400, linestyle='--', c='black')\n",
    "plt.annotate('Past this line:\\ntrain_score - test_score <= 0.01', xy=(1450, 0.7))\n",
    "\n",
    "\n",
    "# Comparing test scores at that line and at max training data (80% of data)\n",
    "plt.scatter(train_sizes[14], test_scores_mean[14], c='orange', s=50)\n",
    "plt.annotate(f\"R2: {round(test_scores_mean[14],2)}\",\n",
    "             xy=(train_sizes[14] + 50, test_scores_mean[14] - 0.03),\n",
    "             fontsize=12, c='orange')\n",
    "\n",
    "plt.scatter(train_sizes[31], test_scores_mean[31], c='orange', s=50)\n",
    "plt.annotate(f\"R2: {round(test_scores_mean[31],2)}\",\n",
    "             xy=(train_sizes[31] - 200, test_scores_mean[31] - 0.03),\n",
    "             fontsize=12, c='orange')\n",
    "\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è The more data, the longer the training. In certain cases, you will be working with enormous datasets. In those situations, the learning curves can help you find the right trade-off between reducing the training size (and training time!) while maintaining a high-performing model.\n",
    "\n",
    "The score at `train_size=1500` is nearly the same as with the full dataset! On the other hand, you could have reduced the computational expense - imagine saving 60% of a 1TB dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá We've seen the evolution of $R^2$ score of our models, but let's see **how accurately the models predict**.\n",
    "\n",
    "‚ùì Calculate the MSE - *Mean Squared Error* (mean of `predictions - target`) - for two models trained on the **full** dataset:\n",
    "\n",
    "1. A model trained on all the features - `mp`, `poss`, pacing and a player's defense/offense ratio\n",
    "2. A model trained with `degree=2` polynomial features\n",
    "\n",
    "‚ùì Save the resulting MSE's into variables called `reg_score` and `poly_score` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What do you observe in comparing the two scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚òëÔ∏è Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('prediction',\n",
    "                         feat_3_score = reg_score,\n",
    "                         poly_3_score = poly_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÅ You did it! Time to commit and push your code\n",
    "\n",
    "Not only did you practice (a lot) of learning curves, but, with sklearn's `PolynomialFeatures`, you used your first *preprocessing transformer* - which is the main topic of the next lecture üôå"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
